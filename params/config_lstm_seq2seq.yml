##############################
# Config for LSTM seq2seq models
##############################

dataset:
  time_list: [9, 12, 15, 18]   # hours; list or 'all'. If 'all', use all hours. The current list = [9, 12, 15, 18]
  train_start: "2016-01-01"  # the start date of the train data; date or 'first'
  train_end: "2023-06-30"  # the end date of the train data; date
  valid_start: "2022-01-01"  # the start date of the validation data; date
  valid_end: "2022-12-31"  # the end date of the validation data; date
  test_start: "2023-07-01"  # the start date of the test data; date
  test_end: "2023-09-30"  # the end date of the train data; date or 'last'
  seq_len_src: 4  # = (num of timestamps in 1 day) * (days) = 8 periods * 1 day; int
  seq_len_tgt: 4  # = (num of timestamps in 1 day) * (days) = 8 periods * 1 day; int
  batch_size: 64  # the num of batch size. len(train_dataset)=1368; int

model:
  hidden_size: 512  # for LSTM layers
  num_layers: 3  # for LSTM layers
  dropout: 0.3  # for LSTM layers

training:
  n_epochs: 2000
  learning_rate: 0.005
  num_features_pred: 10  # the num of features to be used for prediction. Currently, 10 locations
  training_prediction: 'recursive'  # predict the next period recursively
  loss_calculation: 'features_pred'  # features_pred or all
  teacher_forcing_ratio: 0.6  # not used
  dynamic_tf: False  # not used

